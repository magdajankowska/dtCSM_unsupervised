This is dtCSM by Axel Soto and Ryan Kiros after modifications by Magda
Jankowska.

The original code was downloaded from
https://github.com/axelsoto/dtCSM_unsupervised (version as of Jul 14, 2016) and
then modified.



The most important modification is scalling of input data in order to calculate
probabilities for original data. This is to deal with the problem that arise
when distances between data points are large, and due to numerical precision it
leads to the situation when Gaussian kernel produces only zeros, which leads to
the situation that probabilities become NaN (because of division by zero), which
then is fixed by replacing these numbers by zeros. Because of that for some data
(especially ones that are highly multi-dimensional) some batches may have even
the whole input P matrix consisting of zeros -- such batches do not influences
fine tuning by dtCSM at all.

This is the same scaling as in dtCSM in ViTA-SSD system version of Dec 8, 2017.

Other modifications are: random shuffling of input data before creating batches
and fixing the situation when a whole column of input data has the same value
(e.g., zero) - which before resulted in error during standarization of data.

Also, optional parameters were added to the main function, for files storing
standarization data and pretrained network.

Pictures 
6000 digits MNIST unsupervised mapping with dtCSM.jpg
6000 digits MNIST unsupervised mapping with dtCSM.tif
are updated - produced with this version of the algorithm.

readme.md was modified:
something that looks like a typo was corrected:
5 are intermixed with 6 and 8 (not 6 and 9 as it was)






